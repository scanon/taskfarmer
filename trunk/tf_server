#!/usr/bin/perl

use Socket;
use IO::Handle;
use IO::File;
use IO::Socket::INET;
use strict;
use Getopt::Long;
use Data::Dumper;

my $input;

Getopt::Long::Configure("pass_through");
my $result = GetOptions ( "i=s" => \$input);

my $inputfile=$input;
$inputfile=~s/.*\///;

# Parameters
#
my $BATCHSIZE=32;
my $TIMEOUT=1800;
my $TIMEOUT_SOCKET;
my $MAXRETRY=8;
my $MAXBUFF=100*1024*1024;  # 100M buffer
my $FLUSHTIME=20;
my $WINDOWTIME=$FLUSHTIME;
my $FR_FILE="fastrecovery.$inputfile";

# Override defaults
$BATCHSIZE=$ENV{BATCHSIZE} if defined $ENV{BATCHSIZE};
$TIMEOUT=$ENV{SERVER_TIMEOUT} if defined $ENV{SERVER_TIMEOUT};
$TIMEOUT_SOCKET=$ENV{SOCKET_TIMEOUT} if defined $ENV{SOCKET_TIMEOUT};
my $template=$ENV{TEMPLATE};
my $statusfile=$ENV{STATUSFILE};

die "Template doesn't exist!" if (defined $template && ! -e $template);

#  Global vars
#
my $progress_buffer='';
my %input;
my %output;
my %job;
my @ondeck;
my @failed;
my $item;
my $offset;
my $index;
my $next_flush=time+$FLUSHTIME;
my $next_status=time+$FLUSHTIME;
my $next_check=time+$TIMEOUT;
my $processed=0;
my $buffer_size=0;
my $chunksize=2*$BATCHSIZE;

my $inputf=new IO::File $input or die "Unable to open input file ($input)\n";

fast_recovery($FR_FILE);

# make the socket
my %sockargs=( 
	Proto=>'tcp',  
	Timeout => $TIMEOUT_SOCKET,
	Listen => 200,
	Reuse => 1);

$sockargs{LocalPort}=$ENV{PORT} if defined $ENV{PORT};

my $sock=new IO::Socket::INET->new(%sockargs) or die "Unable to create socket\n";

if (defined $ENV{SOCKFILE}){
  open(SF,"> $ENV{SOCKFILE}" ) or die "Unable to open socket file\n";
  print SF $sock->sockport()."\n";
  close SF;
}

my $item=0;
my $remaining_jobs=1;
my $remaining_inputs=1;
my $shutdown=0;
my $ident;
my $command;
my $counters;

initialize_counters($counters);
my @stati=stat $inputf;
$counters->{size}=$stati[7];

open(PROGRESS,">> ./progress.$inputfile");
open(ERROR,">> ./error.$inputfile");
open(LOG,">> ./log.$inputfile");
select LOG; $|=1;
select STDOUT;
$SIG{INT} = \&catch_int;  # best strategy

# This is the main work loop.
while ($remaining_jobs || $remaining_inputs){
  my $new_sock=$sock->accept();
  if (defined $new_sock){
    my $clientaddr=$new_sock->peerhost();
    my $status=do_request($new_sock);

    close $new_sock;
  }
  check_timeouts() if (time>$next_check);
  flush_output() if (time>$next_flush || $buffer_size > $MAXBUFF);
  if (time>$next_status){
    print stderr "dumping status\n";
    update_counters($counters,\%job,\%input,\@ondeck,$statusfile);
    delete_olddata(\%job,\%input);
  }
  
  $remaining_inputs=(scalar @ondeck);
  if ( eof($inputf) || $shutdown){
    $shutdown=1;                      # In case eof got us here.
    $remaining_jobs=remaining_jobs(\%job);             # How much pending stuff is there?
    print LOG "Draining: $remaining_jobs remaining connections.  $remaining_inputs remaining inputs\n";
  }
}
update_counters($counters,\%job,\%input,\@ondeck,$statusfile);

check_inputs(@ondeck);

print LOG "Doing final flush\n";
flush_output();
foreach my $file (keys %output){
  $output{$file}->{handle}->close() if defined $output{$file}->{handle};
}
print LOG "All done\n";
close PRROGRESS;
close ERROR;
close LOG;

# Interrupt handler
#
sub catch_int {
  my $signame = shift;
  print stderr "Caught signal $signame ($shutdown)\n";
  sleep 10 if $shutdown eq 2;
  if ($shutdown){
    flush_output();
    foreach my $file (keys %output){
      $output{$file}->{handle}->close() if defined $output{$file}->{handle};
    }
    print LOG "Exiting\n";
    close PRROGRESS;
    close ERROR;
    close PROGRESS;
    close LOG;
    exit;
  }
  else{
    $shutdown=2;
    flush_output();
    $remaining_jobs=remaining_jobs(\%job);
    print LOG "Shutting down on signal $signame\n";
    print LOG "Draining: $remaining_jobs remaining connections\n";
    $shutdown=1;
  }
}

sub do_request{
  my $sock=shift;
  my $clientaddr=$sock->peerhost();

#  print "Connect from $clientaddr\n";

  my $got_response=0;
  my $status=0;
  $ident="noid";
# Read from client.  Process requests and reponse.
#
  while(<$sock>){
# print $_;
    if (/^RESULTS /){
      my ($command,$jstep)=split;
      chomp $jstep;
      my $expected=defined $job{$jstep};
      my $lines=0;
      while (<$sock>){
        last if /^DONE$/;
        $lines+=read_file($sock,$_,$expected) if /^FILE /;
      }
      if (defined $job{$jstep}){
        $job{$jstep}->{lines}=$lines;
      }
      print $sock "RECEIVED $jstep\n";
      my $status=process_results($jstep);
      print stderr "Unexpected report from $clientaddr:$ident for $jstep\n" if $status eq 0;
    }   #
    elsif (/^IDENT /){
      ($command,$ident)=split;
    }
    elsif (/^NEXT$/){
      if ($shutdown && ! $remaining_inputs){
        print $sock "SHUTDOWN\n";
      }
      send_work($sock);
      last;
    }
    elsif (/^ARGS$/){
      foreach my $a (@ARGV){
        print $sock "$a\n";
      }
      print $sock "DONE\n";
    }
    elsif (/^MESSAGE /){
      chomp; 
      s/MESSAGE //;
      print stderr "MESSAGE: $_\n";
    }
    elsif (/^ERROR /){
      my ($command,$jstep)=split;
      print stderr "ERROR: Job step $jstep\n";
      print ERROR "ERROR: Job step $jstep\n";
      print $sock "RECEIVED $jstep\n";
      $counters->{errors}++;
      requeue_job($jstep);
    }
    else{
      print stderr "Recieved unusual response from $clientaddr: $_";
    }
  }

  return $status;
}

# Read file output from client
#
sub read_file{
  my $sock=shift;
  $_=shift;
  my $write=shift;

  my $clientaddr=$sock->peerhost();
  my $lines=0;
  my ($command,$file)=split;
  while(<$sock>){
     last if /^DONE$/;
     next unless $write;   # should we write out stderr?
     if ( $file eq "stdout"){
       print stdout $_;
     }
     elsif ( $file eq "stderr"){
       print stderr $_;
     }
     else{
       $output{$file}->{buffer}.=$_;
     }
     $lines++;
  }
  $buffer_size+=length $output{$file}->{buffer};
  return $lines;
}

# Process results from client.
# Add line to progress buffer.
# Cleanup data structures.
# (This doesn't actually spool the output)
#
sub process_results{
  my $jstep=shift;

  if (defined ($job{$jstep})){
    my $inputs=join ",",@{$job{$jstep}->{list}};
    my $rtime=time-$job{$jstep}->{start};
    $job{$jstep}->{time}=$rtime;
    $job{$jstep}->{finish}=time;
    $job{$jstep}->{ident}=$ident;
    $progress_buffer.=sprintf "%s %s %d %d %d %d\n", 
	$inputs,$ident,$rtime
	$job{$jstep}->{lines},time,$job{$jstep}->{len};
    printf LOG "Recv: %d input:%25s hostid:%-10s  time:%-4ds lines: %-6d proc: %d\n", 
	$jstep, substr($inputs,0,25),$ident,
	$rtime,$job{$jstep}->{lines},$processed;
    foreach my $inputid (@{$job{$jstep}->{list}}){
      $input{$inputid}->{status}='completed';
    }
    $processed+=$job{$jstep}->{count};
#    delete $job{$jstep};
    return 1;
  }
  else{
    return 0;
  }
}

sub send_work{
  my $new_sock=shift;

  my $sent=[];
  my $length;
  my $ct=0;
  my @list=build_list($BATCHSIZE);

# Send the list if there is one.
#
  if (scalar @list > 0){
    print $new_sock "STEP: $item\n";
    foreach my $inputid (@list){
      print $new_sock $input{$inputid}->{input};
      $input{$inputid}->{status}='in progress';
      push @{$sent}, $inputid;
      $length+=length $input{$inputid}->{input};
      $ct++;
    }
# Save info about the job step.
#
    $job{$item}->{start}=time;
    $job{$item}->{finish}=0;
    $job{$item}->{time}=0;
    $job{$item}->{len}=$length;
    $job{$item}->{list}=$sent; 
    $job{$item}->{count}=$ct; 
    print LOG "Sent: $item hostid:$ident length:$length\n";
    $item++;
  }
  else{
# If no work then send a shutdown
    print $new_sock "SHUTDOWN";
  }
}

# Flush output, progress, and create fast_recovery file
# This tries to keep everything in a consistent state.
#
sub flush_output{
  foreach my $file (keys %output){
    next if ($file eq 'stdout' || $file eq 'stderr');
    if ( ! defined $output{$file}->{handle} ){
      print LOG "Opening new file $file\n";
      $output{$file}->{handle}=new IO::File ">> $file";
      die "Unable to open file $file\n" if ! defined $output{$file}->{handle};
    }
    my $handle=$output{$file}->{handle};
    printf LOG "Flushed %ld bytes to %s\n",length $output{$file}->{buffer},$file;
    print {$handle} $output{$file}->{buffer};
    $handle->flush();
    $output{$file}->{buffer}='';
  }
  flush LOG;
  print PROGRESS $progress_buffer; 
  flush PROGRESS;
  flush ERROR;
  $progress_buffer='';
  $buffer_size=0;

  my $ct=write_fastrecovery($FR_FILE);
  printf LOG "Wrote fast recovery (%d items)\n", $ct;
  $next_flush=time+$FLUSHTIME;
}

#
# This builds up a work list of args inputs.
# It will read in more input if there isn't enough ondeck.
#
sub build_list{
  my $batchsize=shift;
  my @list;
  my @tlist;
  my $ct=0;


# Build rest from ondeck
#
  if (scalar @ondeck < ($batchsize-$ct)){
    @tlist=read_input($inputf,$chunksize);
    $index+=scalar @tlist;
    push @ondeck, @tlist;
  }
  while ($ct<$batchsize && scalar @ondeck >0){
    push @list,shift @ondeck;
    $ct++;
  }
  return @list;
}

sub remaining_jobs{
  my $j=shift;
  my $c=0;
  foreach my $jid (keys %{$j}){
    next if $j->{$jid}->{finish};
    $c++;
  }
  print stderr "Remaining jobs: $c\n";
  return $c;
}

# Look for old inflight messages.
# Move to retry queue
#
sub check_timeouts{
  my $time=time-$TIMEOUT;
  foreach my $jstep (keys %job){
    next if $job{$jstep}->{finish};
    if ($job{$jstep}->{start}<$time){
      print LOG "RETRY: $jstep timed out.  Adding to retry.\n";
      requeue_job($jstep);
      $counters->{timeouts}++;
    }
  }
  $next_check=time+$TIMEOUT/2;
}

# Take inputs for job step
# and put back on the queue.
#
sub requeue_job{
  my $jstep=shift;

  foreach my $inputid (@{$job{$jstep}->{list}}){
    $input{$inputid}->{retry}++;
    printf LOG "Retrying %s for %d time\n",$inputid,$input{$inputid}->{retry};
    if ($input{$inputid}->{retry} < $MAXRETRY){
      unshift @ondeck,$inputid;
      $input{$inputid}->{status}='retry';
    }
    else{
      print stderr "ERROR:  $inputid hit max retries\n";
      print ERROR "FAILED: $inputid hit max retries\n";
      push @failed,$inputid;
    }
  }
  delete $job{$jstep};
}

#
# Read in $read number of inputs from $in.
# If $read is 0 then read until the eof.
# Store input and return list.
#
sub read_input{
  my $in=shift;
  my $read=shift;
  my $ct=0;
  my $l=0;
  my $id;
  my @list;

  return @list if eof($in);
  while(<$in>){
    die "Bad start: $_" if ($l eq 0 && ! /^>/);
    if (/^>/){
      $ct++;
      last if ($read && $ct>$read);
      $id=(tell($in)-length($_));
      $index++;
      my ($bl,$header,$rest)=split /[> \r\n]/;
      $input{$id}->{header}=$header;
      $input{$id}->{input}=$_;
      $input{$id}->{retry}=0;
      $input{$id}->{offset}=$id;
      $input{$id}->{index}=$index;
      $input{$id}->{status}='ondeck';
      push @list,$id;
    }
    else{
       $input{$id}->{input}.=$_;
    }
    $l++;
  }
  my $length=length $_;
  seek $in, -$length,1 or die "Unable to step back: $length";
   
  return @list;
}

#
# Read fast recovery file
# Figure out where we were in the input stream.
# Requeue any outstanding work.
#
sub fast_recovery{
  my $filename=shift; 
  return unless ( -e $filename);
  print STDERR "Recoverying using $filename\n";
  my $fr=new IO::File($filename) or die "Unable to open $filename\n";
# Read the max index and offset
#
  $_=<$fr>;
  $_=~s/.*max: //;  
  ($index,$offset)=split;
  my @offsets=<$fr>;
  foreach (@offsets){
    seek $inputf,$_,0 or die "Unable to seek to input file location $_\n";
    die "Invalid offset: $_ is larger than $offset\n" if ($_>$offset);
    push @ondeck,read_input($inputf,1);
  }
  seek $inputf,$offset,0 or die "Unable to seek to input file location\n";
  printf LOG "Recovered %d inputs from $filename\n", scalar @ondeck;
}

sub check_inputs{
  foreach my $inputid (@_){
    print stderr "Bad inputid: $inputid\n" if (!defined $inputid || $inputid eq '^$');
    die "Bad input in retry $inputid\n\n$input{$inputid}->{input}\n" unless $input{$inputid}->{input}=~/^>/;
  }
}

# Write the fastrecovery file.
# The first line is the index number and the offset into the
#   query file.
# This is followed by a list of inputs that were in process
# This list must include retries, pending jobs, and ondeck.
# The last is needed because the file pointer has already moved past
#   the ondeck list of inputs.
#
sub write_fastrecovery{
  my $filename=shift;
  my $offset;
  my @recoverylist;

  open(FR,"> $filename.new");
#  $offset=tell($inputf)-length($input{$next_header}->{input});
  $offset=tell($inputf);
  $offset=tell($inputf) if (eof($inputf));
  printf FR "# max: %ld %ld\n",$index,$offset;
  my $inputid;
  my $ct=0;

# Add failed jobs to the recovery list

  push @recoverylist,@failed;

# What's in progress
  foreach my $jstep (keys %job){
    next if $job{$jstep}->{finish};
    foreach $inputid (@{$job{$jstep}->{list}}){
      push @recoverylist,$inputid;
    }
  }
  push @recoverylist, @ondeck;

  check_inputs(@recoverylist);
  foreach my $inputid (@recoverylist){
#    print FR $input{$inputid}->{input};
    printf FR "%d\n",$input{$inputid}->{offset};
    $ct++;
  }
  close FR; 
# Try to safely move the file in place.
#
  unlink $filename;
  link $filename.".new", $filename or die "Unable to move $filename.new\n";
  unlink $filename.".new";
  return $ct;
}


sub initialize_counters{
  my $c=shift; 
  my @list=('bytes_in','bytes_out','timeouts','errors');
  for my $field (@list){
    $c->{$field}=0;
  }
}

sub update_counters{
  my $c=shift;
  my $j=shift;
  my $i=shift;
  my $od=shift;
  my $cf=shift;
 
  $c->{bytes_in}=tell $inputf;
  $c->{inflight}=keys %{$j};
  $c->{ondeck}=scalar @{$od};

  $output=open(CF,"> $cf.new");
  print CF "\n\n[Jobs]\n" if $output;
  foreach my $jid (keys %{$j}){
    printf CF "jobid: %d (finish=%s)\n",$jid,$j->{$jid}->{finish} if $output;
  }

  my $inflight=0;
  print CF "\n\n[inflight]\n" if $output;
  foreach my $id (sort keys %{$i}){
    print CF "input: $id $i->{$id}->{status}\n" if $output;
    $inflight++ if $i->{$id}->{status} eq 'in progress';
  }
  $c->{inflight}=$inflight;
  printf CF "\n\n[Global]\n" if $output;
  foreach (sort keys %{$c}){
    printf CF "%s:%d\n",$_,$c->{$_} if $output;
  }
  close CF;
  open(DUMP,"> data.dump");
  print DUMP Dumper($c,$j,$i,$od) if $output;
  close DUMP;

}

sub delete_olddata{
  my $j=shift;
  my $i=shift;
  foreach my $jid (keys %{$j}){
    delete $j->{$jid} if $j->{$jid}->{finish} > time+120 ;
  }
  foreach my $id (sort keys %{$i}){
    delete $i->{$id} if $i->{$id} && $i->{$id}->{status} eq 'completed';
  }
}
